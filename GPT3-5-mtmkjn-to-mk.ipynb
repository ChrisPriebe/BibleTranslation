{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attempt to fine-tune GPT3.5 to translate between languages\n",
    "The first language will be our made up language (see notes on get_bible as to why we think GPT is cheating)\n",
    "The thesis is that fine-tuning will cause GPT3 to learn the fake language as a new language by assigning the embeddings of the\n",
    "new words essentially to the same dimensional space as the english.  Should be trivial to translate it back as it is a word \n",
    "for word translation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.config import get_config\n",
    "from lib.cipher import substitution_cipher\n",
    "import json, random, time\n",
    "\n",
    "# To install pip install pandas, openai, nltk\n",
    "import pandas as pd\n",
    "import openai # !pip install openai==0.27.9\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "FILENAME = \"GPT3-5\"\n",
    "EPOCHS = 1  # Since we are repeating the data with different versions we don't want to overfit\n",
    "SOURCE_BOOKS = ['MAT','LUK','JHN']\n",
    "TARGET_BOOKS = ['MRK']\n",
    "VERSIONS = ['eng-web', 'eng-asv', 'eng-kjv2006']\n",
    "SPLIT_RATIO = 0.8\n",
    "EXPERIMENT_NAME = \"mt_lk_jn_to_mk_test1\"  # Max 18 characters\n",
    "FILENAME = FILENAME + \"_\" + EXPERIMENT_NAME\n",
    "\n",
    "# set environment variable in ipython notebook\n",
    "os.environ[\"OPENAI_API_KEY\"] = get_config('openai')['api_key']\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def system_message(source):\n",
    "    return f\"\"\"You are an expert translator. When the user gives you input from {source} translate it to Birrig.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "bible = pd.read_csv('data/bible.bbe.csv')\n",
    "\n",
    "# Assuming the gospels have a lot of overlap include the synoptic gospels plus John to train\n",
    "# then predict Mark (which should be easy as Matthew and Luke may have copied from him)\n",
    "train = bible[bible['book'].isin(SOURCE_BOOKS)]\n",
    "test = bible[bible['book'].isin(TARGET_BOOKS)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_file(df, file_handlers, versions, split=None):\n",
    "    \n",
    "        results = []\n",
    "        # loop through the train and validate dataframes and add each row to a dataset\n",
    "        for _, row in df.iterrows():\n",
    "            # Loop through all the Bible Versions\n",
    "            for item in versions:\n",
    "                if not pd.isna(row[item]) and not pd.isna(row['birrig']):\n",
    "                    # Create a GPT chat message we will teach GPT how to reply to\n",
    "                    # Thus learning the new language\n",
    "                    line = {\n",
    "                        \"messages\": [\n",
    "                            {\"role\": \"system\", \"content\": system_message(item)},\n",
    "                            {\"role\": \"user\", \"content\": row[item]},\n",
    "                            {\"role\": \"assistant\", \"content\": row['birrig']},\n",
    "                        ]\n",
    "                    }\n",
    "                    # Add to results for now b/c I want to shuffle them\n",
    "                    results.append((random.random(), line))\n",
    "        \n",
    "        results.sort(key=lambda x: x[0])\n",
    "        \n",
    "        # Remove trailing newline in file by this little cheat\n",
    "        optional_newline = [\"\",\"\"]\n",
    "        for some_random_num, line in results:\n",
    "            # Write to the file, if split to train (index 0) or validate (index 1)\n",
    "            if split is None or some_random_num <= split or len(file_handlers) == 1:\n",
    "                index = 0\n",
    "            else:\n",
    "                index = 1\n",
    "            file_handlers[index].write(optional_newline[index] + json.dumps(line))\n",
    "            optional_newline[index] = \"\\n\"\n",
    "\n",
    "with open(f'data/{FILENAME}_train.jsonl','w') as f1, open(f'data/{FILENAME}_validate.jsonl','w') as f2, open(f'data/{FILENAME}_test.jsonl','w') as f3:\n",
    "    write_file(train, [f1,f2], VERSIONS, SPLIT_RATIO)\n",
    "    write_file(test, [f3], VERSIONS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': 'file-SNmsBgd45vwo2U2xwYy3kczA',\n",
       " 'validate': 'file-YPbpsM5BMiIsGe6s71tsMqRc'}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "\n",
    "files = {}\n",
    "for part in ['train', 'validate']:\n",
    "    try:\n",
    "        res = openai.File.create(\n",
    "            file=open(f'data/{FILENAME}_{part}.jsonl', \"r\"),\n",
    "            purpose='fine-tune'\n",
    "        )\n",
    "        files[part] = res['id']\n",
    "    except Exception as e:\n",
    "        print(e, part, f'data/{FILENAME}_{part}.jsonl')\n",
    "\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File is still being processed. Retrying in 30 seconds...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<FineTuningJob fine_tuning.job id=ftjob-s2ioXkXWCfvMyN55IITkNw0Q at 0x10ea11220> JSON: {\n",
       "  \"object\": \"fine_tuning.job\",\n",
       "  \"id\": \"ftjob-s2ioXkXWCfvMyN55IITkNw0Q\",\n",
       "  \"model\": \"gpt-3.5-turbo-0613\",\n",
       "  \"created_at\": 1694562454,\n",
       "  \"finished_at\": null,\n",
       "  \"fine_tuned_model\": null,\n",
       "  \"organization_id\": \"org-dSLF9Ay5XJvsvCOjYOjUYfQQ\",\n",
       "  \"result_files\": [],\n",
       "  \"status\": \"created\",\n",
       "  \"validation_file\": \"file-YPbpsM5BMiIsGe6s71tsMqRc\",\n",
       "  \"training_file\": \"file-SNmsBgd45vwo2U2xwYy3kczA\",\n",
       "  \"hyperparameters\": {\n",
       "    \"n_epochs\": 1\n",
       "  },\n",
       "  \"trained_tokens\": null,\n",
       "  \"error\": null\n",
       "}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "while True:\n",
    "    try:\n",
    "        res = openai.FineTuningJob.create(\n",
    "            training_file=files['train'],\n",
    "            validation_file=files['validate'],\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            hyperparameters={\n",
    "                \"n_epochs\":EPOCHS,\n",
    "            },\n",
    "            suffix=EXPERIMENT_NAME[0:18],\n",
    "        )\n",
    "        job_id = res[\"id\"]\n",
    "\n",
    "        break\n",
    "    except openai.error.InvalidRequestError as e:\n",
    "        if \"File 'file-\" in str(e) and \"' is still being processed and is not ready to be used for fine-tuning.\" in str(e):\n",
    "            print(\"File is still being processed. Retrying in 30 seconds...\")\n",
    "            time.sleep(30)\n",
    "        else:\n",
    "            raise e\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "....................................."
     ]
    }
   ],
   "source": [
    "\n",
    "while True:\n",
    "    res = openai.FineTuningJob.retrieve(job_id)\n",
    "    if res[\"finished_at\"] != None:\n",
    "        print(res)\n",
    "        break\n",
    "    else:\n",
    "        print(\".\", end=\"\")\n",
    "        time.sleep(100)\n",
    "\n",
    "ft_model = res[\"fine_tuned_model\"]\n",
    "ft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_messages(text, version=\"eng-web\"):\n",
    "    # if text is a dict we can assume they already formatted it\n",
    "    if isinstance(text, dict):\n",
    "        # make sure the last item in messages is not a user message\n",
    "        if text['messages'][-1]['role'] == 'user':\n",
    "            text['messages'].pop()\n",
    "        return text\n",
    "    \n",
    "    line = {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": system_message(version)},\n",
    "            {\"role\": \"user\", \"content\": text},\n",
    "        ]\n",
    "    }\n",
    "    return line\n",
    "\n",
    "def translate(text, version=\"eng-web\", debug=False, temperature=0.1):\n",
    "    \"\"\"\n",
    "    Translate text to Birrig\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    text: str|array\n",
    "        The text to translate. If an array is passed, each element will be translated.\n",
    "    version: str\n",
    "        The version of the Bible to translate from. Default is 'eng-web'\n",
    "\n",
    "    Returns\n",
    "    ------- \n",
    "    array\n",
    "        An array of translations\n",
    "        \n",
    "    \"\"\"\n",
    "    if isinstance(text, list):\n",
    "        if len(text) > 20:\n",
    "            # break it into chunks of 20 and call translate on each chunk\n",
    "            # then combine the results\n",
    "            result = []\n",
    "            for i in range(0, len(text), 20):\n",
    "                result += translate(text[i:i+20])\n",
    "            return result\n",
    "\n",
    "    messages = create_messages(text, version)\n",
    "    response = openai.ChatCompletion.create(\n",
    "        engine=ft_model,\n",
    "        messages=messages,\n",
    "        temperature=temperature,  # 0.1 is very little randomness/creativity, 2 is very\n",
    "        max_tokens=2000,\n",
    "        n=1,\n",
    "        logprobs=debug and 5 or 0,\n",
    "    )\n",
    "    result = []\n",
    "    for choice in response.get('choices',[{}]):\n",
    "        text = choice.get('message', {\"content\":\"\"}).get(\"content\").strip()\n",
    "        decoded = substitution_cipher(text, encode=False).strip()\n",
    "        result.append(decoded)\n",
    "        if debug:\n",
    "            # convert logprobs to probabilities\n",
    "            logprobs = choice.get('logprobs', {}).get('token_logprobs', [])\n",
    "            tokens = [substitution_cipher(x, encode=False) for x in choice.get('logprobs', {}).get('tokens', [])]\n",
    "            probs = [10**logprob for logprob in logprobs]\n",
    "            # merge the probabilities with the tokens\n",
    "            probs = list(zip(tokens, probs))\n",
    "\n",
    "            print(\"PROBS: \", probs)\n",
    "            print(\"TRANSLATION: \", text)\n",
    "            print(\"DECODED: \", decoded)\n",
    "            # result.append({\n",
    "        #     'translation': choice.get('text').strip(),\n",
    "        #     'decoded_translation': decoded,\n",
    "        #     'bleu_score': sentence_bleu([text.split()], decoded.split())\n",
    "        # })\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate(\"he ate locusts and honey\", debug=True, temperature=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate(bible[bible['0']=='MRK 1:6'][['eng-web']].values[0][0], debug=False, temperature=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the column eng-web from the first 3 rows of validate then send it to translate\n",
    "test['translation'] = translate(test['eng-web'].to_list())\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop rows that have a null value in any column\n",
    "test = test.dropna()\n",
    "test['bleu_score'] = test.apply(lambda row: sentence_bleu([row['eng-web'].split(), row['eng-asv'].split(), row['eng-kjv2006'].split(),row['engBBE'].split()], row['translation'].split()), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# describe the bleu score column\n",
    "test['bleu_score'].describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "\n",
    "| Stat | Number\n",
    "| ----- | ----- |\n",
    "| Average bleu score |  |\n",
    "| 75 percentile |  |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# foreach row in validate, print the column \"0\" and translation\n",
    "for index, row in test.iterrows():\n",
    "    print(row['0'], row['translation'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handpicked Tests for experimentation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate(\"For God so loved the world that he gave his only Son, so that everyone who believes in him may not die but have eternal life.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate(\"he said let there be light and there was light\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try some unknown words\n",
    "translate(\"Bongo bongo I love you, gone to Venus with a hole in my shoe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate (\"Ship Pit! Pirate ahoy-lay!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate(\"Sheep went baa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate(\"Boat a brother on a mountain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate(\"Be kind and play with rocks!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate(\"SOS! Ship overboard!! Lost ninty percent of people!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
