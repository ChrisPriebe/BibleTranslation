{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attempt to fine-tune GPT3.5 to translate between languages\n",
    "The first language will be our made up language (see notes on get_bible as to why we think GPT is cheating)\n",
    "The thesis is that fine-tuning will cause GPT3 to learn the fake language as a new language by assigning the embeddings of the\n",
    "new words essentially to the same dimensional space as the english.  Should be trivial to translate it back as it is a word \n",
    "for word translation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.config import get_config\n",
    "from lib.cipher import substitution_cipher\n",
    "import json, random, time, os\n",
    "\n",
    "# To install pip install pandas, openai, nltk\n",
    "import pandas as pd\n",
    "import openai # !pip install openai==0.27.9\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "FILENAME = \"GPT3-5\"\n",
    "EPOCHS = 3  # Since we are repeating the data with different versions we don't want to overfit\n",
    "SOURCE_BOOKS = ['MAT','LUK','JHN']\n",
    "TARGET_BOOKS = ['MRK']\n",
    "VERSIONS = ['eng-web', 'eng-asv', 'eng-kjv2006']\n",
    "SPLIT_RATIO = 0.8\n",
    "EXPERIMENT_NAME = \"mt_lk_jn_to_mk_test1\"  # Max 18 characters\n",
    "FILENAME = FILENAME + \"_\" + EXPERIMENT_NAME\n",
    "SEPERATOR = \"\\n---\\n\"\n",
    "\n",
    "# set environment variable in ipython notebook\n",
    "os.environ[\"OPENAI_API_KEY\"] = get_config('openai')['api_key']\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def system_message(source):\n",
    "    return f\"\"\"You are an expert translator. When the user gives you input from {source} translate it to Birrig.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "bible = pd.read_csv('data/bible.bbe.csv')\n",
    "\n",
    "# Assuming the gospels have a lot of overlap include the synoptic gospels plus John to train\n",
    "# then predict Mark (which should be easy as Matthew and Luke may have copied from him)\n",
    "train = bible[bible['book'].isin(SOURCE_BOOKS)]\n",
    "test = bible[bible['book'].isin(TARGET_BOOKS)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_file(df, file_handlers, versions, split=None):\n",
    "    \n",
    "        results = []\n",
    "        # loop through the train and validate dataframes and add each row to a dataset\n",
    "        for _, row in df.iterrows():\n",
    "            # Loop through all the Bible Versions\n",
    "            for item in versions:\n",
    "                if not pd.isna(row[item]) and not pd.isna(row['birrig']):\n",
    "                    # Create a GPT chat message we will teach GPT how to reply to\n",
    "                    # Thus learning the new language\n",
    "                    line = {\n",
    "                        \"messages\": [\n",
    "                            {\"role\": \"system\", \"content\": system_message(item)},\n",
    "                            {\"role\": \"user\", \"content\": row[item]},\n",
    "                            {\"role\": \"assistant\", \"content\": row['birrig']},\n",
    "                        ]\n",
    "                    }\n",
    "                    # Add to results for now b/c I want to shuffle them\n",
    "                    results.append((random.random(), line))\n",
    "        \n",
    "        results.sort(key=lambda x: x[0])\n",
    "        \n",
    "        # Remove trailing newline in file by this little cheat\n",
    "        optional_newline = [\"\",\"\"]\n",
    "        for some_random_num, line in results:\n",
    "            # Write to the file, if split to train (index 0) or validate (index 1)\n",
    "            if split is None or some_random_num <= split or len(file_handlers) == 1:\n",
    "                index = 0\n",
    "            else:\n",
    "                index = 1\n",
    "            file_handlers[index].write(optional_newline[index] + json.dumps(line))\n",
    "            optional_newline[index] = \"\\n\"\n",
    "\n",
    "with open(f'data/{FILENAME}_train.jsonl','w') as f1, open(f'data/{FILENAME}_validate.jsonl','w') as f2, open(f'data/{FILENAME}_test.jsonl','w') as f3:\n",
    "    write_file(train, [f1,f2], VERSIONS, SPLIT_RATIO)\n",
    "    write_file(test, [f3], VERSIONS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': 'file-O4pTTX6oyp37UpfljotjizoL',\n",
       " 'validate': 'file-RqOm21EnUVLA7m22agALfD3b'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "\n",
    "files = {}\n",
    "for part in ['train', 'validate']:\n",
    "    try:\n",
    "        res = openai.File.create(\n",
    "            file=open(f'data/{FILENAME}_{part}.jsonl', \"r\"),\n",
    "            purpose='fine-tune'\n",
    "        )\n",
    "        files[part] = res['id']\n",
    "    except Exception as e:\n",
    "        print(e, part, f'data/{FILENAME}_{part}.jsonl')\n",
    "\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File is still being processed. Retrying in 30 seconds...\n",
      "File is still being processed. Retrying in 30 seconds...\n",
      "File is still being processed. Retrying in 30 seconds...\n",
      "File is still being processed. Retrying in 30 seconds...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<FineTuningJob fine_tuning.job id=ftjob-K1x7qq5sFG4tsASJCSu2AxAC at 0x126837ea0> JSON: {\n",
       "  \"object\": \"fine_tuning.job\",\n",
       "  \"id\": \"ftjob-K1x7qq5sFG4tsASJCSu2AxAC\",\n",
       "  \"model\": \"gpt-3.5-turbo-0613\",\n",
       "  \"created_at\": 1694568819,\n",
       "  \"finished_at\": null,\n",
       "  \"fine_tuned_model\": null,\n",
       "  \"organization_id\": \"org-dSLF9Ay5XJvsvCOjYOjUYfQQ\",\n",
       "  \"result_files\": [],\n",
       "  \"status\": \"created\",\n",
       "  \"validation_file\": \"file-RqOm21EnUVLA7m22agALfD3b\",\n",
       "  \"training_file\": \"file-O4pTTX6oyp37UpfljotjizoL\",\n",
       "  \"hyperparameters\": {\n",
       "    \"n_epochs\": 3\n",
       "  },\n",
       "  \"trained_tokens\": null,\n",
       "  \"error\": null\n",
       "}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "while True:\n",
    "    try:\n",
    "        res = openai.FineTuningJob.create(\n",
    "            training_file=files['train'],\n",
    "            validation_file=files['validate'],\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            hyperparameters={\n",
    "                \"n_epochs\":EPOCHS,\n",
    "            },\n",
    "            suffix=EXPERIMENT_NAME[0:18],\n",
    "        )\n",
    "        job_id = res[\"id\"]\n",
    "\n",
    "        break\n",
    "    except openai.error.InvalidRequestError as e:\n",
    "        if \"File 'file-\" in str(e) and \"' is still being processed and is not ready to be used for fine-tuning.\" in str(e):\n",
    "            print(\"File is still being processed. Retrying in 30 seconds...\")\n",
    "            time.sleep(30)\n",
    "        else:\n",
    "            raise e\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..{\n",
      "  \"object\": \"fine_tuning.job\",\n",
      "  \"id\": \"ftjob-K1x7qq5sFG4tsASJCSu2AxAC\",\n",
      "  \"model\": \"gpt-3.5-turbo-0613\",\n",
      "  \"created_at\": 1694568819,\n",
      "  \"finished_at\": 1694572774,\n",
      "  \"fine_tuned_model\": \"ft:gpt-3.5-turbo-0613:personal:mt-lk-jn-to-mk-tes:7yA4tWKQ\",\n",
      "  \"organization_id\": \"org-dSLF9Ay5XJvsvCOjYOjUYfQQ\",\n",
      "  \"result_files\": [\n",
      "    \"file-im2or2CnzKAmd5szF4Xs3YN8\"\n",
      "  ],\n",
      "  \"status\": \"succeeded\",\n",
      "  \"validation_file\": \"file-RqOm21EnUVLA7m22agALfD3b\",\n",
      "  \"training_file\": \"file-O4pTTX6oyp37UpfljotjizoL\",\n",
      "  \"hyperparameters\": {\n",
      "    \"n_epochs\": 3\n",
      "  },\n",
      "  \"trained_tokens\": 2583870,\n",
      "  \"error\": null\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'ft:gpt-3.5-turbo-0613:personal:mt-lk-jn-to-mk-tes:7yA4tWKQ'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "while True:\n",
    "    res = openai.FineTuningJob.retrieve(job_id)\n",
    "    if res[\"finished_at\"] != None:\n",
    "        print(res)\n",
    "        break\n",
    "    else:\n",
    "        print(\".\", end=\"\")\n",
    "        time.sleep(100)\n",
    "\n",
    "ft_model = res[\"fine_tuned_model\"]\n",
    "ft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_messages(text, version=\"eng-web\"):\n",
    "    # if text is a dict we can assume they already formatted it\n",
    "    if isinstance(text, dict):\n",
    "        # make sure the last item in messages is not a user message\n",
    "        if text['messages'][-1]['role'] == 'user':\n",
    "            text['messages'].pop()\n",
    "        return text\n",
    "\n",
    "    if isinstance(text, list):\n",
    "        text = SEPERATOR.join(text)\n",
    "    \n",
    "    line = {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": system_message(version)},\n",
    "            {\"role\": \"user\", \"content\": text},\n",
    "        ]\n",
    "    }\n",
    "    return line['messages']\n",
    "\n",
    "def translate(text, version=\"eng-web\", debug=False, temperature=0.1):\n",
    "    \"\"\"\n",
    "    Translate text to Birrig\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    text: str|array\n",
    "        The text to translate. If an array is passed, each element will be translated.\n",
    "    version: str\n",
    "        The version of the Bible to translate from. Default is 'eng-web'\n",
    "\n",
    "    Returns\n",
    "    ------- \n",
    "    array\n",
    "        An array of translations\n",
    "        \n",
    "    \"\"\"\n",
    "    if isinstance(text, list):\n",
    "        if len(text) > 20:\n",
    "            # break it into chunks of 20 and call translate on each chunk\n",
    "            # then combine the results\n",
    "            result = []\n",
    "            for i in range(0, len(text), 20):\n",
    "                result += translate(text[i:i+20])\n",
    "            return result\n",
    "\n",
    "    messages = create_messages(text, version)\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=ft_model,\n",
    "        messages=messages,\n",
    "        temperature=temperature,  # 0.1 is very little randomness/creativity, 2 is very\n",
    "        max_tokens=2000,\n",
    "        n=1,\n",
    "        #logprobs=debug and 5 or 0,\n",
    "    )\n",
    "    result = []\n",
    "    for choice in response.get('choices',[{}]):\n",
    "        text = choice.get('message', {\"content\":\"\"}).get(\"content\").strip()\n",
    "        decoded = substitution_cipher(text, encode=False).strip()\n",
    "        if isinstance(text, list):\n",
    "            decoded = decoded.split(SEPERATOR)\n",
    "        result.append(decoded)\n",
    "        \n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['And he took knings and honep.']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate(\"he ate locusts and honey\", debug=True, temperature=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Jesus said, Get youbes for one amother and be my discipres.']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate([\"Jesus said\", \"love one another\", \"be my disciples\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"And John had a coat of camap's hain and a let of talent away abaut his backe. His food was clear and honey.\"]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate(bible[bible['0']=='MRK 1:6'][['eng-web']].values[0][0], debug=False, temperature=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidRequestError",
     "evalue": "['The beginning of the Good News of Jesus Christ, the Son of God.', 'As it is written in the prophets, “Behold, I send my messenger before your face, who will prepare your way before you:', 'the voice of one crying in the wilderness, ‘Make ready the way of the Lord! Make his paths straight!’”', 'John came baptizing in the wilderness and preaching the baptism of repentance for forgiveness of sins.', 'All the country of Judea and all those of Jerusalem went out to him. They were baptized by him in the Jordan river, confessing their sins.', 'John was clothed with camel’s hair and a leather belt around his waist. He ate locusts and wild honey.', 'He preached, saying, “After me comes he who is mightier than I, the thong of whose sandals I am not worthy to stoop down and loosen.', 'I baptized you in water, but he will baptize you in the Holy Spirit.”', 'In those days, Jesus came from Nazareth of Galilee, and was baptized by John in the Jordan.', 'Immediately coming up from the water, he saw the heavens parting and the Spirit descending on him like a dove.', 'A voice came out of the sky, “You are my beloved Son, in whom I am well pleased.”', 'Immediately the Spirit drove him out into the wilderness.', 'He was there in the wilderness forty days, tempted by Satan. He was with the wild animals; and the angels were serving him.', 'Now after John was taken into custody, Jesus came into Galilee, preaching the Good News of God’s Kingdom,', 'and saying, “The time is fulfilled, and God’s Kingdom is at hand! Repent, and believe in the Good News.”', 'Passing along by the sea of Galilee, he saw Simon and Andrew, the brother of Simon, casting a net into the sea, for they were fishermen.', 'Jesus said to them, “Come after me, and I will make you into fishers for men.”', 'Immediately they left their nets, and followed him.', 'Going on a little further from there, he saw James the son of Zebedee, and John his brother, who were also in the boat mending the nets.', 'Immediately he called them, and they left their father, Zebedee, in the boat with the hired servants, and went after him.'] is not of type 'string' - 'messages.1.content'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidRequestError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Get the column eng-web from the first 3 rows of validate then send it to translate\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m test[\u001b[39m'\u001b[39m\u001b[39mtranslation\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m translate(test[\u001b[39m'\u001b[39;49m\u001b[39meng-web\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mto_list())\n\u001b[1;32m      3\u001b[0m test\n",
      "Cell \u001b[0;32mIn[9], line 40\u001b[0m, in \u001b[0;36mtranslate\u001b[0;34m(text, version, debug, temperature)\u001b[0m\n\u001b[1;32m     38\u001b[0m         result \u001b[39m=\u001b[39m []\n\u001b[1;32m     39\u001b[0m         \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, \u001b[39mlen\u001b[39m(text), \u001b[39m20\u001b[39m):\n\u001b[0;32m---> 40\u001b[0m             result \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m translate(text[i:i\u001b[39m+\u001b[39;49m\u001b[39m20\u001b[39;49m])\n\u001b[1;32m     41\u001b[0m         \u001b[39mreturn\u001b[39;00m result\n\u001b[1;32m     43\u001b[0m messages \u001b[39m=\u001b[39m create_messages(text, version)\n",
      "Cell \u001b[0;32mIn[9], line 44\u001b[0m, in \u001b[0;36mtranslate\u001b[0;34m(text, version, debug, temperature)\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[39mreturn\u001b[39;00m result\n\u001b[1;32m     43\u001b[0m messages \u001b[39m=\u001b[39m create_messages(text, version)\n\u001b[0;32m---> 44\u001b[0m response \u001b[39m=\u001b[39m openai\u001b[39m.\u001b[39;49mChatCompletion\u001b[39m.\u001b[39;49mcreate(\n\u001b[1;32m     45\u001b[0m     model\u001b[39m=\u001b[39;49mft_model,\n\u001b[1;32m     46\u001b[0m     messages\u001b[39m=\u001b[39;49mmessages,\n\u001b[1;32m     47\u001b[0m     temperature\u001b[39m=\u001b[39;49mtemperature,  \u001b[39m# 0.1 is very little randomness/creativity, 2 is very\u001b[39;49;00m\n\u001b[1;32m     48\u001b[0m     max_tokens\u001b[39m=\u001b[39;49m\u001b[39m2000\u001b[39;49m,\n\u001b[1;32m     49\u001b[0m     n\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[1;32m     50\u001b[0m     \u001b[39m#logprobs=debug and 5 or 0,\u001b[39;49;00m\n\u001b[1;32m     51\u001b[0m )\n\u001b[1;32m     52\u001b[0m result \u001b[39m=\u001b[39m []\n\u001b[1;32m     53\u001b[0m \u001b[39mfor\u001b[39;00m choice \u001b[39min\u001b[39;00m response\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mchoices\u001b[39m\u001b[39m'\u001b[39m,[{}]):\n",
      "File \u001b[0;32m~/projects/BibleTranslation/venv/lib/python3.8/site-packages/openai/api_resources/chat_completion.py:25\u001b[0m, in \u001b[0;36mChatCompletion.create\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m     24\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 25\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mcreate(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     26\u001b[0m     \u001b[39mexcept\u001b[39;00m TryAgain \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     27\u001b[0m         \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m time\u001b[39m.\u001b[39mtime() \u001b[39m>\u001b[39m start \u001b[39m+\u001b[39m timeout:\n",
      "File \u001b[0;32m~/projects/BibleTranslation/venv/lib/python3.8/site-packages/openai/api_resources/abstract/engine_api_resource.py:153\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m    128\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate\u001b[39m(\n\u001b[1;32m    129\u001b[0m     \u001b[39mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams,\n\u001b[1;32m    137\u001b[0m ):\n\u001b[1;32m    138\u001b[0m     (\n\u001b[1;32m    139\u001b[0m         deployment_id,\n\u001b[1;32m    140\u001b[0m         engine,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    150\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams\n\u001b[1;32m    151\u001b[0m     )\n\u001b[0;32m--> 153\u001b[0m     response, _, api_key \u001b[39m=\u001b[39m requestor\u001b[39m.\u001b[39;49mrequest(\n\u001b[1;32m    154\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mpost\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    155\u001b[0m         url,\n\u001b[1;32m    156\u001b[0m         params\u001b[39m=\u001b[39;49mparams,\n\u001b[1;32m    157\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    158\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    159\u001b[0m         request_id\u001b[39m=\u001b[39;49mrequest_id,\n\u001b[1;32m    160\u001b[0m         request_timeout\u001b[39m=\u001b[39;49mrequest_timeout,\n\u001b[1;32m    161\u001b[0m     )\n\u001b[1;32m    163\u001b[0m     \u001b[39mif\u001b[39;00m stream:\n\u001b[1;32m    164\u001b[0m         \u001b[39m# must be an iterator\u001b[39;00m\n\u001b[1;32m    165\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(response, OpenAIResponse)\n",
      "File \u001b[0;32m~/projects/BibleTranslation/venv/lib/python3.8/site-packages/openai/api_requestor.py:298\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest\u001b[39m(\n\u001b[1;32m    278\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    279\u001b[0m     method,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    286\u001b[0m     request_timeout: Optional[Union[\u001b[39mfloat\u001b[39m, Tuple[\u001b[39mfloat\u001b[39m, \u001b[39mfloat\u001b[39m]]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    287\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[39mbool\u001b[39m, \u001b[39mstr\u001b[39m]:\n\u001b[1;32m    288\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequest_raw(\n\u001b[1;32m    289\u001b[0m         method\u001b[39m.\u001b[39mlower(),\n\u001b[1;32m    290\u001b[0m         url,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    296\u001b[0m         request_timeout\u001b[39m=\u001b[39mrequest_timeout,\n\u001b[1;32m    297\u001b[0m     )\n\u001b[0;32m--> 298\u001b[0m     resp, got_stream \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response(result, stream)\n\u001b[1;32m    299\u001b[0m     \u001b[39mreturn\u001b[39;00m resp, got_stream, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi_key\n",
      "File \u001b[0;32m~/projects/BibleTranslation/venv/lib/python3.8/site-packages/openai/api_requestor.py:700\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response\u001b[0;34m(self, result, stream)\u001b[0m\n\u001b[1;32m    692\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[1;32m    693\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_interpret_response_line(\n\u001b[1;32m    694\u001b[0m             line, result\u001b[39m.\u001b[39mstatus_code, result\u001b[39m.\u001b[39mheaders, stream\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    695\u001b[0m         )\n\u001b[1;32m    696\u001b[0m         \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m parse_stream(result\u001b[39m.\u001b[39miter_lines())\n\u001b[1;32m    697\u001b[0m     ), \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    698\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m--> 700\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response_line(\n\u001b[1;32m    701\u001b[0m             result\u001b[39m.\u001b[39;49mcontent\u001b[39m.\u001b[39;49mdecode(\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m    702\u001b[0m             result\u001b[39m.\u001b[39;49mstatus_code,\n\u001b[1;32m    703\u001b[0m             result\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m    704\u001b[0m             stream\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    705\u001b[0m         ),\n\u001b[1;32m    706\u001b[0m         \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    707\u001b[0m     )\n",
      "File \u001b[0;32m~/projects/BibleTranslation/venv/lib/python3.8/site-packages/openai/api_requestor.py:765\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response_line\u001b[0;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[1;32m    763\u001b[0m stream_error \u001b[39m=\u001b[39m stream \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39merror\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m resp\u001b[39m.\u001b[39mdata\n\u001b[1;32m    764\u001b[0m \u001b[39mif\u001b[39;00m stream_error \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39m200\u001b[39m \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m rcode \u001b[39m<\u001b[39m \u001b[39m300\u001b[39m:\n\u001b[0;32m--> 765\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle_error_response(\n\u001b[1;32m    766\u001b[0m         rbody, rcode, resp\u001b[39m.\u001b[39mdata, rheaders, stream_error\u001b[39m=\u001b[39mstream_error\n\u001b[1;32m    767\u001b[0m     )\n\u001b[1;32m    768\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "\u001b[0;31mInvalidRequestError\u001b[0m: ['The beginning of the Good News of Jesus Christ, the Son of God.', 'As it is written in the prophets, “Behold, I send my messenger before your face, who will prepare your way before you:', 'the voice of one crying in the wilderness, ‘Make ready the way of the Lord! Make his paths straight!’”', 'John came baptizing in the wilderness and preaching the baptism of repentance for forgiveness of sins.', 'All the country of Judea and all those of Jerusalem went out to him. They were baptized by him in the Jordan river, confessing their sins.', 'John was clothed with camel’s hair and a leather belt around his waist. He ate locusts and wild honey.', 'He preached, saying, “After me comes he who is mightier than I, the thong of whose sandals I am not worthy to stoop down and loosen.', 'I baptized you in water, but he will baptize you in the Holy Spirit.”', 'In those days, Jesus came from Nazareth of Galilee, and was baptized by John in the Jordan.', 'Immediately coming up from the water, he saw the heavens parting and the Spirit descending on him like a dove.', 'A voice came out of the sky, “You are my beloved Son, in whom I am well pleased.”', 'Immediately the Spirit drove him out into the wilderness.', 'He was there in the wilderness forty days, tempted by Satan. He was with the wild animals; and the angels were serving him.', 'Now after John was taken into custody, Jesus came into Galilee, preaching the Good News of God’s Kingdom,', 'and saying, “The time is fulfilled, and God’s Kingdom is at hand! Repent, and believe in the Good News.”', 'Passing along by the sea of Galilee, he saw Simon and Andrew, the brother of Simon, casting a net into the sea, for they were fishermen.', 'Jesus said to them, “Come after me, and I will make you into fishers for men.”', 'Immediately they left their nets, and followed him.', 'Going on a little further from there, he saw James the son of Zebedee, and John his brother, who were also in the boat mending the nets.', 'Immediately he called them, and they left their father, Zebedee, in the boat with the hired servants, and went after him.'] is not of type 'string' - 'messages.1.content'"
     ]
    }
   ],
   "source": [
    "# Get the column eng-web from the first 3 rows of validate then send it to translate\n",
    "test['translation'] = translate(test['eng-web'].to_list())\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop rows that have a null value in any column\n",
    "test = test.dropna()\n",
    "test['bleu_score'] = test.apply(lambda row: sentence_bleu([row['eng-web'].split(), row['eng-asv'].split(), row['eng-kjv2006'].split(),row['engBBE'].split()], row['translation'].split()), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# describe the bleu score column\n",
    "test['bleu_score'].describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "\n",
    "| Stat | Number\n",
    "| ----- | ----- |\n",
    "| Average bleu score |  |\n",
    "| 75 percentile |  |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# foreach row in validate, print the column \"0\" and translation\n",
    "for index, row in test.iterrows():\n",
    "    print(row['0'], row['translation'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handpicked Tests for experimentation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate(\"For God so loved the world that he gave his only Son, so that everyone who believes in him may not die but have eternal life.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate(\"he said let there be light and there was light\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try some unknown words\n",
    "translate(\"Bongo bongo I love you, gone to Venus with a hole in my shoe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate (\"Ship Pit! Pirate ahoy-lay!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate(\"Sheep went baa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate(\"Boat a brother on a mountain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate(\"Be kind and play with rocks!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate(\"SOS! Ship overboard!! Lost ninty percent of people!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
